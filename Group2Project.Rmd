---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

Group 2 Members:
Andrew Shooman
Xiangyu Zeng
Thao Nguyen
Yunzhou Liu
   
```{r, warning=FALSE, message=FALSE}
library(readr)
library(mosaic)
library(leaps)
library(car)
source("./ShowSubsets.R")
houses = read_csv("AmesTrain2.csv")
```

Exclude all the categorical variables from this dataset.

```{r}
quantitative.house <- houses[c(2:4, 7:10, 17:19, 23:30, 32:33, 35:36, 39:42)]
head(quantitative.house)
```

These following codes allow us to plot the variables to see the relationships between different variables.

```{r, echo=FALSE}
plot(quantitative.house$Price~., data=quantitative.house[c(2:26)])
```

## Part 1. Create an initial basic model

*build a model using Backward elimination, Forward selection, and Stepwise selection*

##### Build model

Showing the RegSubsets tells us that the best model with 15 variables is:
```
lm(Price ~ 
  LotFrontage + LotArea + Quality + Condition + YearBuilt + YearRemodel + 
  BasementFinSF + BasementSF + FirstSF + SecondSF + Bedroom + Fireplaces + 
  GarageSF + EnclosedPorchSF + ScreenPorchSF, data=quantitative.house[2:26])
```
with a Mallow's CP = 9.28

From the summary of this model, we can see that its Adjusted R-squared is 0.8614, 
with predictors `Bedroom`, `Fireplaces`, `EnclosedPorchSF` insignificant at 5% level.
```{r}
all=regsubsets(quantitative.house$Price~., data=quantitative.house[c(2:26)], nbest=2, nvmax=26)
ShowSubsets(all)
regsubsets.model = lm(Price ~ 
  LotFrontage + LotArea + Quality + Condition + YearBuilt + YearRemodel + 
  BasementFinSF + BasementSF + FirstSF + SecondSF + Bedroom + Fireplaces + 
  GarageSF + EnclosedPorchSF + ScreenPorchSF, data=quantitative.house)
summary(regsubsets.model)
```

We could also create models and compare them by performing backward & forward & stepwise selection. 
Based on all of the result below, 
we can see that the RegSubsets model appears to be same as Backward Selection model, 
while Stepwise and Forward selection appears to create the same model. 
However, we can also observe that these models each has predictors that are not significant at a 5% level:

- Backward elimination model has three predictors insignificant at a 5% level: `Bedroom`, `Fireplaces`, and `EnclosedPorchSF`. 
- Forward Selection model has one predictor insignificant at a 5% level: `EnclosedPorchSF`. However, we should notice that `Bedroom` and `Fireplaces` are significant at 5%, but not 1%. 
- Stepwise selection appears to be the same as Forward Selection. 
```{r}
# Performs a Backward Elimination
Full = lm(quantitative.house$Price~., data=quantitative.house[c(2:26)])
MSE=(summary(Full)$sigma)^2
backward.elimination = step(Full,scale=MSE, trace=FALSE)
# Performs a Forward Selection
none = lm(quantitative.house$Price~1, data=quantitative.house[2:26])
forward.selection = step(none, scope=list(upper=Full), scale=MSE, direction="forward", trace=FALSE)
# Performs a Stepwise Selection
stepwise.selection = step(none, scope=list(upper=Full), scale=MSE, trace=FALSE)
summary(backward.elimination)
summary(forward.selection)
summary(stepwise.selection)
```

##### Selection of models
Comparing these two models, we can see that there is a predictor with VIF = 4 in regsubsets.model, 
which indicates that this predictor might be already predicted by other variables in this model. 
```{r}
vif(stepwise.selection)
"--------"
vif(regsubsets.model)
```
Hence, based on the result above, we would like to choose the Stepwise Selection model 
since overall its predictors have small VIF, compared to regsubsets model. 

##### Summary and VIF for this model

The summary of this model shows that there is only one predictor which is not significant at 5% level: `EnclosedPorchSF`,
which has Pr(>|t|) = 0.064829. 

However, we should be aware that there are two predictors `Bedroom` and `Fireplaces` 
which are significant at 5% level, but not at 1% level. 

The VIF table below shows that every predictor has a VIF less than 5, 
indicating that these predictors are not predicted by other predictors 
and they appear to have little collinearity issue. 
```{r}
summary(stepwise.selection)
vif(stepwise.selection)
```


## Part 2. Residual Analysis for basic model

*Based off model from backward selection*

The Residuals vs Fitted plot show a significant curved relationship in the fitted plot, 
indicicating that the prediction is different based off of the price of the house, 
violating one of the linear model conditions (Zero-Mean). 

Also, the normal Q-Q plot has some extreme values at the ends, 
further showing that the residuals are not constant throughout the model,
violating the Independence condition. 

However, all of the points fall within Cook's distance, as we can see from the Cook's Distance plot
indicating that none of the points have leverage significant enought to drastically change the model.

```{r}
# summary(stepwise.selection)
plot(stepwise.selection, c(1, 2, 5), id.n=5)
```

The maximum and minimum residual values of the model also have significantly large standardized and studentized values,
which indicates that these two points both have the **potential** to have influence over the model,
though they still are within Cooks Distance.

In fact, we can see that there are several points with large standardized and studentized residuals.
We can identify and remove them to see what the model would look like without these points.
```{r}
range(stepwise.selection$residuals)
max.resid = which.max(stepwise.selection$residuals)
min.resid = which.min(stepwise.selection$residuals)
max.resid
min.resid
rstandard(stepwise.selection)[max.resid]
rstudent(stepwise.selection)[max.resid]
rstandard(stepwise.selection)[min.resid]
rstudent(stepwise.selection)[min.resid]
```

We can see that the Q-Q plot approaches the normal Q-Q Plot after subsetting. 
And the re-trained model has a higher Adjusted R-squared, 0.8738, compared to the original 0.8611.

```{r}
sub.house = quantitative.house[c(1:187, 189:197, 199:600),]
reduced.mod = lm(formula = sub.house$Price ~ Quality + GroundSF + 
    BasementSF + YearBuilt + BasementFinSF + GarageSF + LotArea + 
    YearRemodel + Condition + ScreenPorchSF + LotFrontage + Fireplaces + 
    Bedroom + EnclosedPorchSF, data = sub.house[2:26])
summary(reduced.mod)
plot(reduced.mod, c(1, 2, 5))
```

## Part 3. "Fancier model"

We then build another model based on the result of Stepwise selection. 

We have taken data transformations: 
1. Predictor: log of LotArea
2. Predictor: log of (2010 - YearBuilt)
3. Predictor: taken into account for: `Total Full Bath Count` and `Total Half Bath Count`
4. Predictor: taken out some predictors insignificant at 10% level: `YearRemodel`, `LotFrontage`, `Bedroom` and `EnclosedPorchSF`
5. Response: square root of Price


This transformation allows us to improve our Adjusted R-squared from 0.8611 to 0.9001
```{r}
old.mod = lm(formula = quantitative.house$Price ~ Quality + GroundSF + 
    BasementSF + YearBuilt + BasementFinSF + GarageSF + LotArea + 
    YearRemodel + Condition + ScreenPorchSF + LotFrontage + Fireplaces + 
    Bedroom + EnclosedPorchSF, data = quantitative.house[2:26])
summary(old.mod)
mod = lm(formula = sqrt(quantitative.house$Price) ~ Quality + GroundSF + 
    BasementSF + log(2010 - YearBuilt) + BasementFinSF + GarageSF + log(LotArea) + 
   Condition + ScreenPorchSF + Fireplaces
   + I(BasementFBath + FullBath) + I(BasementHBath + HalfBath), data = quantitative.house[2:26])
summary(mod)
```

## Part 4. Residual Analysis for fancier model

After transformation, the Adjusted R-square value became larger from 0.8611 to 0.8996.

We can use the redisuals vs fits plot to check zero mean and constant variance. 

We can see that the residuals of the model have constant variance instead of specific shape, and the mean of them is around 0 since they are along with the line more or less. This suggests that the variances of the error terms are equal. 
No one residual "stands out" from the basic random pattern of residuals. This suggests that there are no outliers.

```{r}
plot(mod$residuals~mod$fitted.values)
abline(a=0,b=0)
mean(mod$residuals)
```



The histogram shows a roughly bell curve shape, and the Q-Q norm plot show that the data is normal distributed. 
After transformation, the Adjusted R-square value became larger 0.8996.

Also, while the normal Q-Q plot has some extreme values at the left ends, 
the right tail of this Q-Q Plot shows little extreme values,
indicating that the residuals for our new model is more normally distributed compared to the previous model. 

```{r}
hist(mod$residuals,breaks=20)
qqnorm(mod$residuals)
qqline(mod$residuals)
```



We can further look at the Cook's Distance plot and see that, 
different from our previous Cook's Distance plot, our new plot shows more centered distribution, 
with no observations having high leverage. 
For instance, our initial model has two points located far away from cluster of data, which are labeled 438 and 102. 
However, on the plot below, we can see that they are no longer far away from our cluster of data.
All five of the most extreme residuals thus lie well within the Cooks Distance, making it highly unlikely any of them are influential on the model.
All of these means that the new model is better than before and our new model doesnâ€™t need additional transformation.

```{r}
plot(mod, 5, id.n=5)
```

The standardized and studentized residual did not vary too much, and likely aren't highly influential to the model.
```{r}
range(mod$residuals)
which.max(mod$residuals)
which.min(mod$residuals)
rstandard(mod)[572]
rstudent(mod)[572]
rstandard(mod)[188]
rstudent(mod)[188]
```



## Part 5. Final Model

The final model is:
```
sqrt(Price) = 
  Quality + GroundSF + BasementSF + log(2010 - YearBuilt) + BasementFinSF + 
  GarageSF + log(LotArea) + Condition + ScreenPorchSF + Fireplaces + 
  I(BasementFBath + FullBath) + I(BasementHBath + HalfBath)
```

And we can predict the price of a new house based on our model. 

The predicted value of the price of the house for the given description is 228.2079 thoudsand dollars.
We are 95% confident that the price of the house with the description in the question is fall into the interval [181.6355, 280.0897]. Therefore, we are 95% confident to say that the predicted price in this model is between 181.6355  and 280.0897 (in 1,000â€™s of dollars).

```{r}
house=data.frame(Quality=7, BasementSF=1150,  YearBuilt=1995,  GarageSF=502, LotArea=11060, YearRemodel=2003, Condition=5, LotFrontage=90, Fireplaces=1, Bedroom=3, GroundSF=2314, BasementFinSF=0, EnclosedPorchSF=0, ScreenPorchSF=0, BasementHBath=0, BasementFBath=0, FullBath=2, HalfBath=1)

# Since we have taken the square root of Price, 
# The prediction result is sqrt(Price), 
# and we need to square it to reveal the actual price.

(predict.lm(mod,house,interval="prediction",level=.95))^2
```


